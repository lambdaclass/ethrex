use crate::sequencer::errors::ProverServerError;
use crate::utils::{
    config::{
        committer::CommitterConfig, errors::ConfigError, eth::EthConfig,
        prover_server::ProverServerConfig,
    },
    prover::{
        errors::SaveStateError,
        proving_systems::{ProofCalldata, ProverType},
        save_state::{StateFileType, StateType, *},
    },
};
use ethrex_common::{
    types::{Block, BlockHeader},
    Address, H160, H256, U256,
};
use ethrex_l2_sdk::calldata::{encode_calldata, Value};
use ethrex_rpc::clients::eth::{eth_sender::Overrides, EthClient, WrappedTransaction};
use ethrex_storage::Store;
use ethrex_storage_l2::StoreL2;
use ethrex_vm::{Evm, EvmError, ExecutionDB};
use secp256k1::SecretKey;
use serde::{Deserialize, Serialize};
use std::{fmt::Debug, net::IpAddr, time::Duration};
use tokio::{
    io::{AsyncReadExt, AsyncWriteExt},
    net::{TcpListener, TcpStream},
    sync::TryAcquireError,
};
use tracing::{debug, error, info, warn};

// These constants have to match with the OnChainProposer.sol contract
const R0VERIFIER: &str = "R0VERIFIER()";
const SP1VERIFIER: &str = "SP1VERIFIER()";
const PICOVERIFIER: &str = "PICOVERIFIER()";
const VERIFIER_CONTRACTS: [&str; 3] = [R0VERIFIER, SP1VERIFIER, PICOVERIFIER];
const DEV_MODE_ADDRESS: H160 = H160([
    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
    0x00, 0x00, 0x00, 0xAA,
]);
const VERIFY_FUNCTION_SIGNATURE: &str =
    "verifyBatch(uint256,bytes,bytes32,bytes32,bytes32,bytes,bytes,bytes32,bytes,uint256[8])";

#[derive(Debug, Serialize, Deserialize, Default)]
pub struct ProverInputData {
    pub blocks: Vec<Block>,
    pub parent_block_header: BlockHeader,
    pub db: ExecutionDB,
}

#[derive(Clone)]
struct ProverServer {
    ip: IpAddr,
    port: u16,
    store: Store,
    eth_client: EthClient,
    on_chain_proposer_address: Address,
    l1_address: Address,
    l1_private_key: SecretKey,
    needed_proof_types: Vec<ProverType>,
    l2_store: StoreL2,
}

/// Enum for the ProverServer <--> ProverClient Communication Protocol.
#[derive(Serialize, Deserialize)]
pub enum ProofData {
    /// 1.
    /// The Client initiates the connection with a Request.
    /// Asking for the ProverInputData the prover_server considers/needs.
    Request,

    /// 2.
    /// The Server responds with a Response containing the ProverInputData.
    /// If the Response will is ProofData::Response{None, None}, the Client knows that the Request couldn't be performed.
    Response {
        batch_number: Option<u64>,
        input: Option<ProverInputData>,
    },

    /// 3.
    /// The Client submits the zk Proof generated by the prover
    /// for the specified batch, as calldata for the verifier contract.
    Submit {
        batch_number: u64,
        calldata: ProofCalldata,
    },

    /// 4.
    /// The Server acknowledges the receipt of the proof and updates its state,
    SubmitAck { batch_number: u64 },
}

impl ProofData {
    /// Builder function for creating a Request
    pub fn request() -> Self {
        ProofData::Request
    }

    /// Builder function for creating a Response
    pub fn response(batch_number: Option<u64>, input: Option<ProverInputData>) -> Self {
        ProofData::Response {
            batch_number,
            input,
        }
    }

    /// Builder function for creating a Submit
    pub fn submit(batch_number: u64, calldata: ProofCalldata) -> Self {
        ProofData::Submit {
            batch_number,
            calldata,
        }
    }

    /// Builder function for creating a SubmitAck
    pub fn submit_ack(batch_number: u64) -> Self {
        ProofData::SubmitAck { batch_number }
    }
}

pub async fn start_prover_server(store: Store, l2_store: StoreL2) -> Result<(), ConfigError> {
    let server_config = ProverServerConfig::from_env()?;
    let eth_config = EthConfig::from_env()?;
    let proposer_config = CommitterConfig::from_env()?;
    let mut prover_server = ProverServer::new_from_config(
        server_config.clone(),
        &proposer_config,
        eth_config,
        store,
        l2_store,
    )
    .await?;
    prover_server.run(&server_config).await;
    Ok(())
}

impl ProverServer {
    pub async fn new_from_config(
        config: ProverServerConfig,
        committer_config: &CommitterConfig,
        eth_config: EthConfig,
        store: Store,
        l2_store: StoreL2,
    ) -> Result<Self, ConfigError> {
        let eth_client = EthClient::new(&eth_config.rpc_url);
        let on_chain_proposer_address = committer_config.on_chain_proposer_address;

        let verifier_contracts = EthClient::get_verifier_contracts(
            &eth_client,
            &VERIFIER_CONTRACTS,
            on_chain_proposer_address,
        )
        .await?;

        let mut needed_proof_types = vec![];
        if !config.dev_mode {
            for (key, addr) in verifier_contracts {
                if addr == DEV_MODE_ADDRESS {
                    continue;
                } else {
                    match key.as_str() {
                        "R0VERIFIER()" => {
                            info!("RISC0 proof needed");
                            needed_proof_types.push(ProverType::RISC0);
                        }
                        "SP1VERIFIER()" => {
                            info!("SP1 proof needed");
                            needed_proof_types.push(ProverType::SP1);
                        }
                        "PICOVERIFIER()" => {
                            info!("PICO proof needed");
                            needed_proof_types.push(ProverType::Pico);
                        }
                        _ => unreachable!("There shouldn't be a value different than the used backends/verifiers R0VERIFIER|SP1VERIFER|PICOVERIFIER."),
                    }
                }
            }
        } else {
            needed_proof_types.push(ProverType::Exec);
        }

        Ok(Self {
            ip: config.listen_ip,
            port: config.listen_port,
            store,
            eth_client,
            on_chain_proposer_address,
            l1_address: config.l1_address,
            l1_private_key: config.l1_private_key,
            needed_proof_types,
            l2_store,
        })
    }

    pub async fn run(&mut self, server_config: &ProverServerConfig) {
        loop {
            match self.clone().main_logic().await {
                Ok(()) => {
                    if !server_config.dev_mode {
                        info!("Prover Server shutting down");
                        break;
                    }
                }
                Err(e) => {
                    let error_message = if !server_config.dev_mode {
                        format!("Prover Server, severe Error, trying to restart the main_logic function: {e}")
                    } else {
                        format!("Prover Server Dev Error: {e}")
                    };
                    error!(error_message);
                }
            }

            tokio::time::sleep(Duration::from_millis(200)).await;
        }
    }

    async fn main_logic(self) -> Result<(), ProverServerError> {
        let (shutdown_tx, mut shutdown_rx) = tokio::sync::oneshot::channel();
        tokio::task::spawn(async move {
            if let Err(e) = tokio::signal::ctrl_c().await {
                error!("Error handling ctrl_c: {e}");
            };
            if let Err(e) = shutdown_tx.send(0) {
                error!("Error sending shutdown message through the oneshot::channel {e}");
            };
        });

        let listener = TcpListener::bind(format!("{}:{}", self.ip, self.port)).await?;

        let concurent_clients = 3;
        let sem = std::sync::Arc::new(tokio::sync::Semaphore::new(concurent_clients));
        info!(
            "Starting TCP server at {}:{} accepting {concurent_clients} concurrent clients.",
            self.ip, self.port
        );

        loop {
            tokio::select! {
                _ = &mut shutdown_rx => {
                    debug!("Shutting down...");
                    // It will return from the main_logic() with an `Ok(())`
                    // And inside the run() function the loop will break
                    // and the prover_server task will finish gracefully.
                    break;
                }
                res = listener.accept() => {
                    match res {
                        Ok((stream, addr)) => {

                            match sem.clone().try_acquire_owned(){
                                Ok(permit) => {
                                    // Cloning the ProverServer structure to use the handle_connection() fn
                                    // in every spawned task.
                                    // The important fields are `Store` and `EthClient`
                                    // Both fields are wrapped with an Arc, making it possible to clone
                                    // the entire structure.
                                    let mut self_clone = self.clone();
                                    tokio::task::spawn(async move {
                                        if let Err(e) = self_clone.handle_connection(stream).await {
                                            error!("Error handling connection from {addr}: {e}");
                                        } else {
                                            debug!("Connection from {addr} handled successfully");
                                        }
                                        drop(permit);
                                    });
                                },
                                Err(e) => {
                                    match e {
                                        TryAcquireError::Closed => error!("Fatal error the semaphore has been closed: {e}"),
                                        TryAcquireError::NoPermits => warn!("Connection limit reached. Closing connection from {addr}."),
                                    }
                                }
                            }

                        }
                        Err(e) => {
                            error!("Failed to accept connection: {e}");
                        }
                    }
                }
            }
        }

        Ok(())
    }

    async fn handle_connection(&mut self, mut stream: TcpStream) -> Result<(), ProverServerError> {
        let mut buffer = Vec::new();
        stream.read_to_end(&mut buffer).await?;

        let last_verified_batch =
            EthClient::get_last_verified_batch(&self.eth_client, self.on_chain_proposer_address)
                .await?;

        let batch_to_verify = last_verified_batch + 1;

        let mut tx_submitted = false;

        // If we have all the proofs send a transaction to verify them on chain
        let send_tx =
            match batch_number_has_all_needed_proofs(batch_to_verify, &self.needed_proof_types) {
                Ok(has_all_proofs) => has_all_proofs,
                Err(e) => {
                    if let SaveStateError::IOError(ref error) = e {
                        if error.kind() != std::io::ErrorKind::NotFound {
                            return Err(e.into());
                        }
                    } else {
                        return Err(e.into());
                    }
                    false
                }
            };

        if send_tx {
            self.handle_proof_submission(batch_to_verify).await?;

            // Remove the Proofs for that batch_number
            match prune_state(batch_to_verify) {
                Ok(_) => (),
                Err(e) => {
                    if let SaveStateError::IOError(ref error) = e {
                        if error.kind() != std::io::ErrorKind::NotFound {
                            return Err(e.into());
                        }
                    } else {
                        return Err(e.into());
                    }
                }
            }

            tx_submitted = true;
        }

        let data: Result<ProofData, _> = serde_json::from_slice(&buffer);
        match data {
            Ok(ProofData::Request) => {
                if let Err(e) = self
                    .handle_request(&mut stream, batch_to_verify, tx_submitted)
                    .await
                {
                    warn!("Failed to handle request: {e}");
                }
            }
            Ok(ProofData::Submit {
                batch_number,
                calldata,
            }) => {
                self.handle_submit(&mut stream, batch_number).await?;

                // Avoid storing a proof of a future batch_number
                // CHECK: maybe we would like to store all the proofs given the case in which
                // the provers generate them fast enough. In this way, we will avoid unneeded reexecution.
                if batch_number != batch_to_verify {
                    return Err(ProverServerError::Custom(format!("Prover Client submitted an invalid batch_number: {batch_number}. The last_proved_block is: {last_verified_batch}")));
                }

                // If the transaction was submitted for the batch_to_verify
                // avoid storing already used proofs.
                if tx_submitted {
                    return Ok(());
                }

                // Check if we have the proof for that ProverType
                let has_proof = match batch_number_has_state_file(
                    StateFileType::Proof(calldata.prover_type),
                    batch_number,
                ) {
                    Ok(has_proof) => has_proof,
                    Err(e) => {
                        let error = format!("{e}");
                        if !error.contains("No such file or directory") {
                            return Err(e.into());
                        }
                        false
                    }
                };
                // If we don't have it, insert it.
                if !has_proof {
                    write_state(batch_number, &StateType::Proof(calldata))?;
                }

                // Then if we have all the proofs, we send the transaction in the next `handle_connection` call.
                self.handle_proof_submission(batch_number).await?;
            }
            Err(e) => {
                warn!("Failed to parse request: {e}");
            }
            _ => {
                warn!("Invalid request");
            }
        }

        debug!("Connection closed");
        Ok(())
    }

    async fn handle_request(
        &self,
        stream: &mut TcpStream,
        batch_number: u64,
        tx_submitted: bool,
    ) -> Result<(), ProverServerError> {
        debug!("Request received");

        let lastest_commited_batch =
            EthClient::get_last_committed_batch(&self.eth_client, self.on_chain_proposer_address)
                .await?;

        let response = if batch_number > lastest_commited_batch {
            let response = ProofData::response(None, None);
            debug!("Didn't send response");
            response
        } else if tx_submitted {
            let response = ProofData::response(None, None);
            debug!("Batch: {batch_number} has been submitted.");
            response
        } else {
            let input = self.create_prover_input(batch_number).await?;
            let response = ProofData::response(Some(batch_number), Some(input));
            info!("Sent Response for batch_number: {batch_number}");
            response
        };

        let buffer = serde_json::to_vec(&response)?;
        stream
            .write_all(&buffer)
            .await
            .map_err(ProverServerError::ConnectionError)?;
        Ok(())
    }

    async fn handle_submit(
        &self,
        stream: &mut TcpStream,
        batch_number: u64,
    ) -> Result<(), ProverServerError> {
        debug!("Submit received for Batch Number: {batch_number}");

        let response = ProofData::submit_ack(batch_number);

        let buffer = serde_json::to_vec(&response)?;
        stream
            .write_all(&buffer)
            .await
            .map_err(ProverServerError::ConnectionError)?;
        Ok(())
    }

    async fn create_prover_input(
        &self,
        batch_number: u64,
    ) -> Result<ProverInputData, ProverServerError> {
        let Some(block_numbers) = self
            .l2_store
            .get_block_numbers_by_batch(batch_number)
            .await?
        else {
            return Err(ProverServerError::ItemNotFoundInStore(format!(
                "Batch number {batch_number} not found in store"
            )));
        };

        let blocks = self.fetch_blocks(block_numbers).await?;

        let parent_hash = blocks
            .first()
            .ok_or_else(|| {
                ProverServerError::Custom("No blocks found for the given batch number".to_string())
            })?
            .header
            .parent_hash;

        let db = Evm::to_execution_db(&self.store.clone(), &blocks)
            .await
            .map_err(EvmError::ExecutionDB)?;

        let parent_block_header = self
            .store
            .get_block_header_by_hash(parent_hash)?
            .ok_or(ProverServerError::StorageDataIsNone)?;

        debug!("Created prover input for batch {batch_number}");

        Ok(ProverInputData {
            db,
            blocks,
            parent_block_header,
        })
    }

    async fn fetch_blocks(&self, block_numbers: Vec<u64>) -> Result<Vec<Block>, ProverServerError> {
        let mut blocks = vec![];
        for block_number in block_numbers {
            let header = self
                .store
                .get_block_header(block_number)?
                .ok_or(ProverServerError::StorageDataIsNone)?;
            let body = self
                .store
                .get_block_body(block_number)
                .await?
                .ok_or(ProverServerError::StorageDataIsNone)?;
            blocks.push(Block::new(header, body));
        }
        Ok(blocks)
    }

    pub async fn handle_proof_submission(
        &self,
        batch_number: u64,
    ) -> Result<H256, ProverServerError> {
        // TODO: change error
        // TODO: If the proof is not needed, a default calldata is used,
        // the structure has to match the one defined in the OnChainProposer.sol contract.
        // It may cause some issues, but the ethrex_prover_lib cannot be imported,
        // this approach is straight-forward for now.
        let risc0_proof = {
            if self.needed_proof_types.contains(&ProverType::RISC0) {
                let risc0_proof =
                    read_proof(batch_number, StateFileType::Proof(ProverType::RISC0))?;
                if risc0_proof.prover_type != ProverType::RISC0 {
                    return Err(ProverServerError::Custom(
                        "RISC0 Proof isn't present".to_string(),
                    ));
                }
                risc0_proof.calldata
            } else {
                ProverType::RISC0.empty_calldata()
            }
        };

        let sp1_proof = {
            if self.needed_proof_types.contains(&ProverType::SP1) {
                let sp1_proof = read_proof(batch_number, StateFileType::Proof(ProverType::SP1))?;
                if sp1_proof.prover_type != ProverType::SP1 {
                    return Err(ProverServerError::Custom(
                        "SP1 Proof isn't present".to_string(),
                    ));
                }
                sp1_proof.calldata
            } else {
                ProverType::SP1.empty_calldata()
            }
        };

        let pico_proof = {
            if self.needed_proof_types.contains(&ProverType::Pico) {
                let pico_proof = read_proof(batch_number, StateFileType::Proof(ProverType::Pico))?;
                if pico_proof.prover_type != ProverType::Pico {
                    return Err(ProverServerError::Custom(
                        "Pico Proof isn't present".to_string(),
                    ));
                }
                pico_proof.calldata
            } else {
                ProverType::Pico.empty_calldata()
            }
        };

        debug!("Sending proof for block number: {batch_number}");

        let calldata_values = [
            &[Value::Uint(U256::from(batch_number))],
            risc0_proof.as_slice(),
            sp1_proof.as_slice(),
            pico_proof.as_slice(),
        ]
        .concat();

        let calldata = encode_calldata(VERIFY_FUNCTION_SIGNATURE, &calldata_values)?;

        let gas_price = self
            .eth_client
            .get_gas_price_with_extra(20)
            .await?
            .try_into()
            .map_err(|_| {
                ProverServerError::InternalError("Failed to convert gas_price to a u64".to_owned())
            })?;

        let verify_tx = self
            .eth_client
            .build_eip1559_transaction(
                self.on_chain_proposer_address,
                self.l1_address,
                calldata.into(),
                Overrides {
                    max_fee_per_gas: Some(gas_price),
                    max_priority_fee_per_gas: Some(gas_price),
                    ..Default::default()
                },
            )
            .await?;

        let mut tx = WrappedTransaction::EIP1559(verify_tx);

        let verify_tx_hash = self
            .eth_client
            .send_tx_bump_gas_exponential_backoff(&mut tx, &self.l1_private_key)
            .await?;

        info!("Sent proof for batch {batch_number}, with transaction hash {verify_tx_hash:#x}");

        Ok(verify_tx_hash)
    }
}
