use crate::sequencer::errors::ProverServerError;
use crate::utils::prover::proving_systems::ProofCalldata;
use crate::utils::prover::save_state::{
    batch_number_has_state_file, write_state, StateFileType, StateType,
};
use crate::{CommitterConfig, EthConfig, ProofCoordinatorConfig, SequencerConfig};
use ethrex_common::{
    types::{Block, BlockHeader},
    Address,
};
use ethrex_rpc::clients::eth::EthClient;
use ethrex_storage::Store;
use ethrex_storage_rollup::StoreRollup;
use ethrex_vm::{Evm, EvmError, ExecutionDB};
use serde::{Deserialize, Serialize};
use std::{fmt::Debug, net::IpAddr};
use tokio::{
    io::{AsyncReadExt, AsyncWriteExt},
    net::{TcpListener, TcpStream},
    sync::TryAcquireError,
};
use tracing::{debug, error, info, warn};

use super::errors::SequencerError;
use super::utils::sleep_random;

#[derive(Debug, Serialize, Deserialize, Default)]
pub struct ProverInputData {
    pub blocks: Vec<Block>,
    pub parent_block_header: BlockHeader,
    pub db: ExecutionDB,
}

#[derive(Clone)]
struct ProofCoordinator {
    listen_ip: IpAddr,
    port: u16,
    store: Store,
    eth_client: EthClient,
    on_chain_proposer_address: Address,
    rollup_store: StoreRollup,
}

/// Enum for the ProverServer <--> ProverClient Communication Protocol.
#[derive(Serialize, Deserialize)]
pub enum ProofData {
    /// 1.
    /// The Client initiates the connection with a BatchRequest.
    /// Asking for the ProverInputData the prover_server considers/needs.
    BatchRequest,

    /// 2.
    /// The Server responds with a BatchResponse containing the ProverInputData.
    /// If the BatchResponse is ProofData::BatchResponse{None, None},
    /// the Client knows the BatchRequest couldn't be performed.
    BatchResponse {
        batch_number: Option<u64>,
        input: Option<ProverInputData>,
    },

    /// 3.
    /// The Client submits the zk Proof generated by the prover
    /// for the specified batch, as calldata for the verifier contract.
    ProofSubmit {
        batch_number: u64,
        calldata: ProofCalldata,
    },

    /// 4.
    /// The Server acknowledges the receipt of the proof and updates its state,
    ProofSubmitACK { batch_number: u64 },
}

impl ProofData {
    /// Builder function for creating a BatchRequest
    pub fn batch_request() -> Self {
        ProofData::BatchRequest
    }

    /// Builder function for creating a BlockResponse
    pub fn batch_response(batch_number: u64, input: ProverInputData) -> Self {
        ProofData::BatchResponse {
            batch_number: Some(batch_number),
            input: Some(input),
        }
    }

    pub fn empty_batch_response() -> Self {
        ProofData::BatchResponse {
            batch_number: None,
            input: None,
        }
    }

    /// Builder function for creating a ProofSubmit
    pub fn proof_submit(batch_number: u64, calldata: ProofCalldata) -> Self {
        ProofData::ProofSubmit {
            batch_number,
            calldata,
        }
    }

    /// Builder function for creating a ProofSubmitAck
    pub fn proof_submit_ack(batch_number: u64) -> Self {
        ProofData::ProofSubmitACK { batch_number }
    }
}

pub async fn start_proof_coordinator(
    store: Store,
    rollup_store: StoreRollup,
    cfg: SequencerConfig,
) -> Result<(), SequencerError> {
    let proof_coordinator = ProofCoordinator::new_from_config(
        &cfg.proof_coordinator,
        &cfg.l1_committer,
        &cfg.eth,
        store,
        rollup_store,
    )
    .await?;
    proof_coordinator.run().await;

    Ok(())
}

impl ProofCoordinator {
    pub async fn new_from_config(
        config: &ProofCoordinatorConfig,
        committer_config: &CommitterConfig,
        eth_config: &EthConfig,
        store: Store,
        rollup_store: StoreRollup,
    ) -> Result<Self, SequencerError> {
        let eth_client = EthClient::new_with_maximum_fees(
            &eth_config.rpc_url,
            eth_config.maximum_allowed_max_fee_per_blob_gas,
            eth_config.maximum_allowed_max_fee_per_blob_gas,
        );
        let on_chain_proposer_address = committer_config.on_chain_proposer_address;

        Ok(Self {
            listen_ip: config.listen_ip,
            port: config.listen_port,
            store,
            eth_client,
            on_chain_proposer_address,
            rollup_store,
        })
    }

    pub async fn run(&self) {
        loop {
            if let Err(err) = self.main_logic().await {
                error!("L1 Proof Coordinator Error: {}", err);
            }

            sleep_random(200).await;
        }
    }

    async fn main_logic(&self) -> Result<(), ProverServerError> {
        let listener = TcpListener::bind(format!("{}:{}", self.listen_ip, self.port)).await?;

        let concurent_clients = 3;
        let sem = std::sync::Arc::new(tokio::sync::Semaphore::new(concurent_clients));
        info!(
            "Starting TCP server at {}:{} accepting {concurent_clients} concurrent clients.",
            self.listen_ip, self.port
        );

        loop {
            let res = listener.accept().await;
            match res {
                Ok((stream, addr)) => {
                    match sem.clone().try_acquire_owned() {
                        Ok(permit) => {
                            // Cloning the ProofCoordinator structure to use the handle_connection() fn
                            // in every spawned task.
                            // The important fields are `Store` and `EthClient`
                            // Both fields are wrapped with an Arc, making it possible to clone
                            // the entire structure.
                            let self_clone = self.clone();
                            tokio::task::spawn(async move {
                                if let Err(e) = self_clone.handle_connection(stream).await {
                                    error!("Error handling connection from {addr}: {e}");
                                } else {
                                    debug!("Connection from {addr} handled successfully");
                                }
                                drop(permit);
                            });
                        }
                        Err(e) => match e {
                            TryAcquireError::Closed => {
                                error!("Fatal error the semaphore has been closed: {e}")
                            }
                            TryAcquireError::NoPermits => {
                                warn!("Connection limit reached. Closing connection from {addr}.")
                            }
                        },
                    }
                }
                Err(e) => {
                    error!("Failed to accept connection: {e}");
                }
            }
        }
    }

    async fn handle_connection(&self, mut stream: TcpStream) -> Result<(), ProverServerError> {
        let mut buffer = Vec::new();
        stream.read_to_end(&mut buffer).await?;

        let data: Result<ProofData, _> = serde_json::from_slice(&buffer);
        match data {
            Ok(ProofData::BatchRequest) => {
                if let Err(e) = self.handle_request(&mut stream).await {
                    error!("Failed to handle BatchRequest: {e}");
                }
            }
            Ok(ProofData::ProofSubmit {
                batch_number,
                calldata,
            }) => {
                if let Err(e) = self
                    .handle_submit(&mut stream, batch_number, calldata)
                    .await
                {
                    error!("Failed to handle ProofSubmit: {e}");
                }
            }
            Err(e) => {
                warn!("Failed to parse request: {e}");
            }
            _ => {
                warn!("Invalid request");
            }
        }

        debug!("Connection closed");
        Ok(())
    }

    async fn handle_request(&self, stream: &mut TcpStream) -> Result<(), ProverServerError> {
        info!("BatchRequest received");

        let batch_to_verify = 1 + self
            .eth_client
            .get_last_verified_batch(self.on_chain_proposer_address)
            .await?;

        let response = if !self.rollup_store.contains_batch(&batch_to_verify).await? {
            let response = ProofData::empty_batch_response();
            debug!("Sending empty BatchResponse");
            response
        } else {
            let input = self.create_prover_input(batch_to_verify).await?;
            let response = ProofData::batch_response(batch_to_verify, input);
            debug!("Sending BatchResponse for block_number: {batch_to_verify}");
            response
        };

        let buffer = serde_json::to_vec(&response)?;
        stream
            .write_all(&buffer)
            .await
            .map_err(ProverServerError::ConnectionError)
            .map(|_| info!("BatchResponse sent for batch number: {batch_to_verify}"))
    }

    async fn handle_submit(
        &self,
        stream: &mut TcpStream,
        batch_number: u64,
        calldata: ProofCalldata,
    ) -> Result<(), ProverServerError> {
        info!("ProofSubmit received for batch number: {batch_number}");

        // Check if we have the proof for that ProverType
        if batch_number_has_state_file(StateFileType::Proof(calldata.prover_type), batch_number)? {
            debug!("Already known proof. Skipping");
        } else {
            write_state(batch_number, &StateType::Proof(calldata))?;
        }

        let response = ProofData::proof_submit_ack(batch_number);

        let buffer = serde_json::to_vec(&response)?;
        stream
            .write_all(&buffer)
            .await
            .map_err(ProverServerError::ConnectionError)
            .map(|_| info!("ProofSubmit ACK sent"))
    }

    async fn create_prover_input(
        &self,
        batch_number: u64,
    ) -> Result<ProverInputData, ProverServerError> {
        // Get blocks in batch
        let Some(block_numbers) = self
            .rollup_store
            .get_block_numbers_by_batch(batch_number)
            .await?
        else {
            return Err(ProverServerError::ItemNotFoundInStore(format!(
                "Batch number {batch_number} not found in store"
            )));
        };

        let blocks = self.fetch_blocks(block_numbers).await?;

        // Create execution_db
        let db = Evm::to_execution_db(&self.store.clone(), &blocks)
            .await
            .map_err(EvmError::ExecutionDB)?;

        // Get the block_header of the parent of the first block
        let parent_hash = blocks
            .first()
            .ok_or_else(|| {
                ProverServerError::Custom("No blocks found for the given batch number".to_string())
            })?
            .header
            .parent_hash;

        let parent_block_header = self
            .store
            .get_block_header_by_hash(parent_hash)?
            .ok_or(ProverServerError::StorageDataIsNone)?;

        debug!("Created prover input for batch {batch_number}");

        Ok(ProverInputData {
            db,
            blocks,
            parent_block_header,
        })
    }

    async fn fetch_blocks(&self, block_numbers: Vec<u64>) -> Result<Vec<Block>, ProverServerError> {
        let mut blocks = vec![];
        for block_number in block_numbers {
            let header = self
                .store
                .get_block_header(block_number)?
                .ok_or(ProverServerError::StorageDataIsNone)?;
            let body = self
                .store
                .get_block_body(block_number)
                .await?
                .ok_or(ProverServerError::StorageDataIsNone)?;
            blocks.push(Block::new(header, body));
        }
        Ok(blocks)
    }
}
