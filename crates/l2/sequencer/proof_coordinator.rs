use crate::sequencer::errors::ProofCoordinatorError;
use crate::sequencer::setup::{prepare_quote_prerequisites, register_tdx_key};
use crate::sequencer::utils::get_latest_sent_batch;
use crate::{
    BlockProducerConfig, CommitterConfig, EthConfig, ProofCoordinatorConfig, SequencerConfig,
};
use bytes::Bytes;
use ethrex_blockchain::Blockchain;
use ethrex_common::types::BlobsBundle;
use ethrex_common::types::block_execution_witness::ExecutionWitness;
use ethrex_common::{
    Address,
    types::{Block, blobs_bundle},
};
use ethrex_l2_common::prover::{BatchProof, ProverType};
use ethrex_metrics::metrics;
use ethrex_rpc::clients::eth::EthClient;
use ethrex_storage::Store;
use ethrex_storage_rollup::StoreRollup;
use futures::StreamExt;
use secp256k1::SecretKey;
use serde::{Deserialize, Serialize};
use serde_with::serde_as;
use spawned_concurrency::messages::Unused;
use spawned_concurrency::tasks::{CastResponse, GenServer, GenServerHandle, spawn_listener};
use std::net::IpAddr;
use std::sync::Arc;
use tokio::{
    io::{AsyncReadExt, AsyncWriteExt},
    net::{TcpListener, TcpStream},
};
use tokio_stream::wrappers::TcpListenerStream;
use tracing::{debug, error, info, warn};

#[cfg(feature = "metrics")]
use ethrex_metrics::l2::metrics::METRICS;
#[cfg(feature = "metrics")]
use std::{collections::HashMap, time::SystemTime};
#[cfg(feature = "metrics")]
use tokio::sync::Mutex;

#[serde_as]
#[derive(Serialize, Deserialize, Clone)]
pub struct ProverInputData {
    pub blocks: Vec<Block>,
    pub db: ExecutionWitness,
    pub elasticity_multiplier: u64,
    #[cfg(feature = "l2")]
    #[serde_as(as = "[_; 48]")]
    pub blob_commitment: blobs_bundle::Commitment,
    #[cfg(feature = "l2")]
    #[serde_as(as = "[_; 48]")]
    pub blob_proof: blobs_bundle::Proof,
}

/// Enum for the ProverServer <--> ProverClient Communication Protocol.
#[allow(clippy::large_enum_variant)]
#[derive(Serialize, Deserialize, Clone)]
pub enum ProofData {
    /// 1.
    /// The client performs any needed setup steps
    /// This includes things such as key registration
    ProverSetup {
        prover_type: ProverType,
        payload: Bytes,
    },

    /// 2.
    /// The Server acknowledges the receipt of the setup and it's completion
    ProverSetupACK,

    /// 3.
    /// The Client initiates the connection with a BatchRequest.
    /// Asking for the ProverInputData the prover_server considers/needs.
    /// The commit hash is used to ensure the client and server are compatible.
    BatchRequest { commit_hash: String },

    /// 4.
    /// The Server responds with an InvalidCodeVersion if the code version is not compatible.
    /// The Client should then update its code to match the server's version.
    InvalidCodeVersion { commit_hash: String },

    /// 5.
    /// The Server responds with a BatchResponse containing the ProverInputData.
    /// If the BatchResponse is ProofData::BatchResponse{None, None},
    /// the Client knows the BatchRequest couldn't be performed.
    BatchResponse {
        batch_number: Option<u64>,
        input: Option<ProverInputData>,
    },

    /// 6.
    /// The Client submits the zk Proof generated by the prover for the specified batch.
    ProofSubmit {
        batch_number: u64,
        batch_proof: BatchProof,
    },

    /// 7.
    /// The Server acknowledges the receipt of the proof and updates its state,
    ProofSubmitACK { batch_number: u64 },
}

impl ProofData {
    /// Builder function for creating a ProverSetup
    pub fn prover_setup(prover_type: ProverType, payload: Bytes) -> Self {
        ProofData::ProverSetup {
            prover_type,
            payload,
        }
    }

    /// Builder function for creating a ProverSetupACK
    pub fn prover_setup_ack() -> Self {
        ProofData::ProverSetupACK
    }

    /// Builder function for creating a BatchRequest
    pub fn batch_request(commit_hash: String) -> Self {
        ProofData::BatchRequest { commit_hash }
    }

    /// Builder function for creating a InvalidCodeVersion
    pub fn invalid_code_version(commit_hash: String) -> Self {
        ProofData::InvalidCodeVersion { commit_hash }
    }

    /// Builder function for creating a BatchResponse
    pub fn batch_response(batch_number: u64, input: ProverInputData) -> Self {
        ProofData::BatchResponse {
            batch_number: Some(batch_number),
            input: Some(input),
        }
    }

    pub fn empty_batch_response() -> Self {
        ProofData::BatchResponse {
            batch_number: None,
            input: None,
        }
    }

    /// Builder function for creating a ProofSubmit
    pub fn proof_submit(batch_number: u64, batch_proof: BatchProof) -> Self {
        ProofData::ProofSubmit {
            batch_number,
            batch_proof,
        }
    }

    /// Builder function for creating a ProofSubmitAck
    pub fn proof_submit_ack(batch_number: u64) -> Self {
        ProofData::ProofSubmitACK { batch_number }
    }
}

pub fn get_commit_hash() -> String {
    env!("VERGEN_GIT_SHA").to_string()
}

#[derive(Clone)]
pub enum ProofCordInMessage {
    Data(ProofData, Arc<TcpStream>),
}

#[derive(Clone, PartialEq)]
pub enum ProofCordOutMessage {
    Done,
}

#[derive(Clone)]
pub struct ProofCoordinator {
    store: Store,
    eth_client: EthClient,
    on_chain_proposer_address: Address,
    elasticity_multiplier: u64,
    rollup_store: StoreRollup,
    rpc_url: String,
    tdx_private_key: Option<SecretKey>,
    blockchain: Arc<Blockchain>,
    validium: bool,
    needed_proof_types: Vec<ProverType>,
    commit_hash: String,
    #[cfg(feature = "metrics")]
    request_timestamp: Arc<Mutex<HashMap<u64, SystemTime>>>,
}

impl ProofCoordinator {
    #[allow(clippy::too_many_arguments)]
    pub async fn new(
        config: &ProofCoordinatorConfig,
        committer_config: &CommitterConfig,
        eth_config: &EthConfig,
        proposer_config: &BlockProducerConfig,
        store: Store,
        rollup_store: StoreRollup,
        blockchain: Arc<Blockchain>,
        needed_proof_types: Vec<ProverType>,
    ) -> Result<Self, ProofCoordinatorError> {
        let eth_client = EthClient::new_with_config(
            eth_config.rpc_url.iter().map(AsRef::as_ref).collect(),
            eth_config.max_number_of_retries,
            eth_config.backoff_factor,
            eth_config.min_retry_delay,
            eth_config.max_retry_delay,
            Some(eth_config.maximum_allowed_max_fee_per_gas),
            Some(eth_config.maximum_allowed_max_fee_per_blob_gas),
        )?;
        let on_chain_proposer_address = committer_config.on_chain_proposer_address;

        let rpc_url = eth_config
            .rpc_url
            .first()
            .ok_or(ProofCoordinatorError::Custom(
                "no rpc urls present!".to_string(),
            ))?
            .to_string();

        Ok(Self {
            store,
            eth_client,
            on_chain_proposer_address,
            elasticity_multiplier: proposer_config.elasticity_multiplier,
            rollup_store,
            rpc_url,
            tdx_private_key: config.tdx_private_key,
            blockchain,
            validium: config.validium,
            needed_proof_types,
            commit_hash: get_commit_hash(),
            #[cfg(feature = "metrics")]
            request_timestamp: Arc::new(Mutex::new(HashMap::new())),
        })
    }

    pub async fn spawn(
        store: Store,
        rollup_store: StoreRollup,
        cfg: SequencerConfig,
        blockchain: Arc<Blockchain>,
        needed_proof_types: Vec<ProverType>,
    ) -> Result<(), ProofCoordinatorError> {
        let state = Self::new(
            &cfg.proof_coordinator,
            &cfg.l1_committer,
            &cfg.eth,
            &cfg.block_producer,
            store,
            rollup_store,
            blockchain,
            needed_proof_types,
        )
        .await?;

        let proof_coordinator = ProofCoordinator::start(state);
        start_prover_listener(
            proof_coordinator,
            cfg.proof_coordinator.listen_ip,
            cfg.proof_coordinator.listen_port,
        )
        .await?;

        Ok(())
    }

    async fn handle_request(
        &mut self,
        stream: &mut TcpStream,
        commit_hash: String,
    ) -> Result<(), ProofCoordinatorError> {
        info!("BatchRequest received");

        if commit_hash != self.commit_hash {
            error!(
                "Code version mismatch: expected {}, got {}",
                self.commit_hash, commit_hash
            );

            let response = ProofData::invalid_code_version(self.commit_hash.clone());
            send_response(stream, &response).await?;
            info!("InvalidCodeVersion sent");
            return Ok(());
        }

        let batch_to_verify = 1 + get_latest_sent_batch(
            self.needed_proof_types.clone(),
            &self.rollup_store,
            &self.eth_client,
            self.on_chain_proposer_address,
        )
        .await
        .map_err(|err| ProofCoordinatorError::InternalError(err.to_string()))?;

        let mut all_proofs_exist = true;
        for proof_type in &self.needed_proof_types {
            if self
                .rollup_store
                .get_proof_by_batch_and_type(batch_to_verify, *proof_type)
                .await?
                .is_none()
            {
                all_proofs_exist = false;
                break;
            }
        }

        let response =
            if all_proofs_exist || !self.rollup_store.contains_batch(&batch_to_verify).await? {
                debug!("Sending empty BatchResponse");
                ProofData::empty_batch_response()
            } else {
                let input = self.create_prover_input(batch_to_verify).await?;
                debug!("Sending BatchResponse for block_number: {batch_to_verify}");
                metrics!(
                    // First request starts a timer until a proof is received. The elapsed time will be
                    // the estimated proving time.
                    // This should be used for development only and runs on the assumption that:
                    //   1. There's a single prover
                    //   2. Communication does not fail
                    //   3. Communication adds negligible overhead in comparison with proving time
                    let mut lock = self.request_timestamp.lock().await;
                    lock.entry(batch_to_verify).or_insert(SystemTime::now());
                );
                ProofData::batch_response(batch_to_verify, input)
            };

        send_response(stream, &response).await?;
        info!("BatchResponse sent for batch number: {batch_to_verify}");

        Ok(())
    }

    async fn handle_submit(
        &mut self,
        stream: &mut TcpStream,
        batch_number: u64,
        batch_proof: BatchProof,
    ) -> Result<(), ProofCoordinatorError> {
        info!("ProofSubmit received for batch number: {batch_number}");

        // Check if we have a proof for this batch and prover type
        let prover_type = batch_proof.prover_type();
        if self
            .rollup_store
            .get_proof_by_batch_and_type(batch_number, prover_type)
            .await?
            .is_some()
        {
            info!(
                ?batch_number,
                ?prover_type,
                "A proof was received for a batch and type that is already stored"
            );
        } else {
            metrics!(
                tracing::warn!("getting request timestamp for batch {batch_number}");
                let mut request_timestamps = self.request_timestamp.lock().await;
                let request_timestamp = request_timestamps.get(&batch_number).ok_or(
                    ProofCoordinatorError::InternalError(
                        "request timestamp could not be found".to_string(),
                    ),
                )?;
                let proving_time = request_timestamp
                    .elapsed()
                    .map_err(|_| ProofCoordinatorError::InternalError("failed to compute proving time".to_string()))?
                    .as_secs().try_into()
                    .map_err(|_| ProofCoordinatorError::InternalError("failed to convert proving time to i64".to_string()))?;
                METRICS.set_batch_proving_time(batch_number, proving_time)?;
                tracing::warn!("removed request timestamp for batch {batch_number}");
                let _ = request_timestamps.remove(&batch_number);
            );
            // If not, store it
            self.rollup_store
                .store_proof_by_batch_and_type(batch_number, prover_type, batch_proof)
                .await?;
        }
        let response = ProofData::proof_submit_ack(batch_number);
        send_response(stream, &response).await?;
        info!("ProofSubmit ACK sent");
        Ok(())
    }

    async fn handle_setup(
        &mut self,
        stream: &mut TcpStream,
        prover_type: ProverType,
        payload: Bytes,
    ) -> Result<(), ProofCoordinatorError> {
        info!("ProverSetup received for {prover_type}");

        match prover_type {
            ProverType::TDX => {
                let Some(key) = self.tdx_private_key.as_ref() else {
                    return Err(ProofCoordinatorError::MissingTDXPrivateKey);
                };
                prepare_quote_prerequisites(
                    &self.eth_client,
                    &self.rpc_url,
                    &hex::encode(key.secret_bytes()),
                    &hex::encode(&payload),
                )
                .await
                .map_err(|e| {
                    ProofCoordinatorError::Custom(format!("Could not setup TDX key {e}"))
                })?;
                register_tdx_key(
                    &self.eth_client,
                    key,
                    self.on_chain_proposer_address,
                    payload,
                )
                .await?;
            }
            _ => {
                warn!("Setup requested for {prover_type}, which doesn't need setup.")
            }
        }

        let response = ProofData::prover_setup_ack();

        send_response(stream, &response).await?;
        info!("ProverSetupACK sent");
        Ok(())
    }

    async fn create_prover_input(
        &mut self,
        batch_number: u64,
    ) -> Result<ProverInputData, ProofCoordinatorError> {
        // Get blocks in batch
        let Some(block_numbers) = self
            .rollup_store
            .get_block_numbers_by_batch(batch_number)
            .await?
        else {
            return Err(ProofCoordinatorError::ItemNotFoundInStore(format!(
                "Batch number {batch_number} not found in store"
            )));
        };

        let blocks = self.fetch_blocks(block_numbers).await?;

        let witness = self
            .blockchain
            .generate_witness_for_blocks(&blocks)
            .await
            .map_err(ProofCoordinatorError::from)?;

        // Get blobs bundle cached by the L1 Committer (blob, commitment, proof)
        let (blob_commitment, blob_proof) = if self.validium {
            ([0; 48], [0; 48])
        } else {
            let blob = self
                .rollup_store
                .get_blobs_by_batch(batch_number)
                .await?
                .ok_or(ProofCoordinatorError::MissingBlob(batch_number))?;
            let BlobsBundle {
                mut commitments,
                mut proofs,
                ..
            } = BlobsBundle::create_from_blobs(&blob)?;
            match (commitments.pop(), proofs.pop()) {
                (Some(commitment), Some(proof)) => (commitment, proof),
                _ => return Err(ProofCoordinatorError::MissingBlob(batch_number)),
            }
        };

        debug!("Created prover input for batch {batch_number}");

        Ok(ProverInputData {
            db: witness,
            blocks,
            elasticity_multiplier: self.elasticity_multiplier,
            #[cfg(feature = "l2")]
            blob_commitment,
            #[cfg(feature = "l2")]
            blob_proof,
        })
    }

    async fn fetch_blocks(
        &mut self,
        block_numbers: Vec<u64>,
    ) -> Result<Vec<Block>, ProofCoordinatorError> {
        let mut blocks = vec![];
        for block_number in block_numbers {
            let header = self
                .store
                .get_block_header(block_number)?
                .ok_or(ProofCoordinatorError::StorageDataIsNone)?;
            let body = self
                .store
                .get_block_body(block_number)
                .await?
                .ok_or(ProofCoordinatorError::StorageDataIsNone)?;
            blocks.push(Block::new(header, body));
        }
        Ok(blocks)
    }
}

impl GenServer for ProofCoordinator {
    type CallMsg = Unused;
    type CastMsg = ProofCordInMessage;
    type OutMsg = ProofCordOutMessage;
    type Error = ProofCoordinatorError;

    async fn handle_cast(
        &mut self,
        message: Self::CastMsg,
        _handle: &GenServerHandle<Self>,
    ) -> CastResponse {
        match message {
            ProofCordInMessage::Data(data, stream) => {
                let Some(mut stream) = Arc::into_inner(stream) else {
                    error!("Failed to send response to prover client failed to get stream");
                    return CastResponse::NoReply;
                };
                match data {
                    ProofData::BatchRequest { commit_hash } => {
                        if let Err(e) = self.handle_request(&mut stream, commit_hash).await {
                            error!("Failed to handle BatchRequest: {e}");
                        }
                    }
                    ProofData::ProofSubmit {
                        batch_number,
                        batch_proof,
                    } => {
                        if let Err(e) = self
                            .handle_submit(&mut stream, batch_number, batch_proof)
                            .await
                        {
                            error!("Failed to handle ProofSubmit: {e}");
                        }
                    }
                    ProofData::ProverSetup {
                        prover_type,
                        payload,
                    } => {
                        if let Err(e) = self.handle_setup(&mut stream, prover_type, payload).await {
                            error!("Failed to handle ProverSetup: {e}");
                        }
                    }
                    _ => {
                        warn!("Invalid request");
                    }
                }
            }
        }
        CastResponse::NoReply
    }
}

async fn send_response(
    stream: &mut TcpStream,
    response: &ProofData,
) -> Result<(), ProofCoordinatorError> {
    let buffer = serde_json::to_vec(response)?;
    stream
        .write_all(&buffer)
        .await
        .map_err(ProofCoordinatorError::ConnectionError)?;
    Ok(())
}

async fn start_prover_listener(
    proof_coordinator: GenServerHandle<ProofCoordinator>,
    ip: IpAddr,
    port: u16,
) -> std::io::Result<()> {
    let listener = TcpListener::bind(format!("{ip}:{port}")).await?;
    let stream = TcpListenerStream::new(listener);
    let stream = stream.filter_map(async |result| match result {
        Ok(mut stream) => {
            let mut buffer = Vec::new();
            stream
                .read_to_end(&mut buffer)
                .await
                .inspect_err(|err| error!("Failed to read from tcp stream: {err}"))
                .ok()?;

            serde_json::from_slice(&buffer)
                .map(|data| ProofCordInMessage::Data(data, Arc::new(stream)))
                .inspect_err(|err| error!("Failed to deserialize data: {}", err))
                .ok()
        }
        Err(e) => {
            error!("{}", e);
            None
        }
    });
    info!("Starting TCP server at {ip}:{port}.");
    spawn_listener(proof_coordinator, stream);
    Ok(())
}
