name: Run Hive Tests

on:
  workflow_call:
    inputs:
      job_type:
        required: true
        type: string
        description: "Allowed values: 'trigger' or 'daily'"
    outputs:
      report_artifact_name:
        description: "The name of the hive report artifact"
        value: ${{ jobs.hive-report.outputs.artifact_name }}

jobs:
  run-hive:
    name: Hive - ${{ matrix.test.name }}
    runs-on: ubuntu-latest
    strategy:
      matrix:
        test:
          - {
              name: "Rpc Compat tests",
              file_name: rpc-compat,
              simulation: ethereum/rpc-compat,
            }
          - { name: "Devp2p eth tests", file_name: devp2p, simulation: devp2p }
          - {
              name: "Engine tests",
              file_name: engine,
              simulation: ethereum/engine,
            }
          - { name: "Sync tests", file_name: sync, simulation: ethereum/sync }
          - {
              name: "Consume Engine tests (Rest)",
              file_name: consume-engine-rest,
              simulation: ethereum/eest/consume-engine,
              limit: ".*fork_Paris.*|.*fork_Shanghai.*|.*fork_Cancun.*",
            }
          - {
              name: "Consume Engine tests (Prague)",
              file_name: consume-engine-prague,
              simulation: ethereum/eest/consume-engine,
              limit: ".*fork_Prague.*",
            }
          - {
              name: "Consume Engine tests (Osaka)",
              file_name: consume-engine-osaka,
              simulation: ethereum/eest/consume-engine,
              limit: ".*fork_Osaka.*",
            }
          - {
              name: "Consume RLP tests (Rest)",
              file_name: consume-rlp-rest,
              simulation: ethereum/eest/consume-rlp,
              limit: ".*fork_Paris.*|.*fork_Shanghai.*|.*fork_Cancun.*",
            }
          - {
              name: "Consume RLP tests (Prague)",
              file_name: consume-rlp-prague,
              simulation: ethereum/eest/consume-rlp,
              limit: ".*fork_Prague.*",
            }
          - {
              name: "Consume RLP tests (Osaka)",
              file_name: consume-rlp-osaka,
              simulation: ethereum/eest/consume-rlp,
              limit: ".*fork_Osaka.*",
            }
          - {
              name: "Execute Blobs tests",
              file_name: execute-blobs,
              simulation: ethereum/eest/execute-blobs,
            }

    steps:
      - name: Verify input parameters
        if: ${{ inputs.job_type != 'trigger' && inputs.job_type != 'daily' }}
        run: |
          echo "Invalid job_type input: ${{ inputs.job_type }}. Allowed values are 'trigger' or 'daily'."
          exit 1

      - name: Checkout sources
        uses: actions/checkout@v4

      # Set custom args defined in Dockerfile to pin execution-spec-tests versions
      # See: https://github.com/ethereum/hive/blob/c2dab60f898b94afe8eeac505f60dcde59205e77/simulators/ethereum/eest/consume-rlp/Dockerfile#L4-L8
      - name: Determine hive flags
        id: hive-flags
        shell: bash
        env:
          SIMULATION: ${{ matrix.test.simulation }}
          SIM_LIMIT: ${{ matrix.test.limit || '' }}
        run: |
          FLAGS='--sim.parallelism 4 --sim.loglevel 1'
          if [[ "$SIMULATION" == "ethereum/eest/consume-engine" || "$SIMULATION" == "ethereum/eest/consume-rlp" ]]; then
            FLAGS+=" --sim.buildarg fixtures=https://github.com/ethereum/execution-spec-tests/releases/download/v5.1.0/fixtures_stable.tar.gz"
            FLAGS+=" --sim.buildarg branch=main"
            FLAGS+=" --client.checktimelimit=180s"
          fi
          if [[ -n "$SIM_LIMIT" ]]; then
            FLAGS+=" --sim.limit \"$SIM_LIMIT\""
          fi
          echo "flags=$FLAGS" >> "$GITHUB_OUTPUT"

      - name: Run Hive Simulation
        id: run-hive-action
        uses: ethpandaops/hive-github-action@v0.5.0
        continue-on-error: true
        with:
          # This uses ethereum/hive as default
          # Uncomment to use a custom hive version
          # hive_repository: lambdaclass/hive
          # hive_version: master
          simulator: ${{ matrix.test.simulation }}
          client: ethrex
          extra_flags: ${{ steps.hive-flags.outputs.flags }}
          workflow_artifact_upload: true
          workflow_artifact_prefix: ${{ matrix.test.file_name }}_${{ inputs.job_type }}

  hive-report:
    name: Generate and Save report
    needs: run-hive
    runs-on: ubuntu-latest
    outputs:
      artifact_name: results_${{ inputs.job_type }}.md
    steps:
      - name: Checkout sources
        uses: actions/checkout@v4
      - name: Setup Rust Environment
        uses: ./.github/actions/setup-rust

      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          path: hive/workspace/logs
          pattern: "*_${{ inputs.job_type }}-results.zip"
          merge-multiple: true

      - name: Flatten hive result files
        shell: bash
        run: |
          shopt -s globstar nullglob
          mkdir -p hive/workspace/logs
          for json in hive/workspace/logs/**/*.json; do
            base_name=$(basename "$json")
            target="hive/workspace/logs/$base_name"
            if [[ "$json" != "$target" ]]; then
              mv "$json" "$target"
            fi
          done
          find hive/workspace/logs -mindepth 1 -type d -exec rm -rf {} +

      - name: Print failed tests
        if: always()
        shell: bash
        run: |
          shopt -s nullglob
          found_failures=false
          for log in hive/workspace/logs/*.log; do
            mapfile -t failures < <(grep 'FAILED ' "$log" | sed -E 's/^.*FAILED ([^ ]+).*/\1/' || true)
            if (( ${#failures[@]} )); then
              found_failures=true
              echo "Failures in $(basename "$log"):"
              printf '  %s\n' "${failures[@]}"
              echo
            fi
          done
          if [ "$found_failures" = false ]; then
            echo "No test failures found in logs."
          fi

      - name: Generate the hive report
        run: cargo run -p hive_report > results.md

      - name: Upload ${{ inputs.job_type }} result
        uses: actions/upload-artifact@v4
        with:
          name: results_${{ inputs.job_type }}.md
          path: results.md
          if-no-files-found: error

      - name: Post results in summary
        run: |
          echo "# Hive coverage report" >> $GITHUB_STEP_SUMMARY
          cat results.md >> $GITHUB_STEP_SUMMARY
